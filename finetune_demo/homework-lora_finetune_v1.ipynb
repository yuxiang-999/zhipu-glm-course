{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 单卡GPU 进行 ChatGLM3-6B模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 硬件需求\n",
    "显存：24GB\n",
    "显卡架构：安培架构（推荐）\n",
    "内存：16GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /media/zr/Data/Code/ChatGLM3/finetune_demo/data/AdvertiseGen\n",
    "\n",
    "接着，运行本代码来切割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T05:02:34.749308Z",
     "start_time": "2024-01-18T05:02:25.564458Z"
    },
    "execution": {
     "iopub.execute_input": "2024-03-26T12:39:11.258063Z",
     "iopub.status.busy": "2024-03-26T12:39:11.257657Z",
     "iopub.status.idle": "2024-03-26T12:39:12.532830Z",
     "shell.execute_reply": "2024-03-26T12:39:12.531800Z",
     "shell.execute_reply.started": "2024-03-26T12:39:11.258023Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调，这里将 `/media/zr/Data/Code/ChatGLM3/venv/bin/python3` 换成你的 python3 的绝对路径以保证正常运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "050b5efb-0968-489f-bfc1-53798b36c70a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T02:46:36.936590Z",
     "iopub.status.busy": "2024-03-27T02:46:36.935879Z",
     "iopub.status.idle": "2024-03-27T02:46:37.062909Z",
     "shell.execute_reply": "2024-03-27T02:46:37.061438Z",
     "shell.execute_reply.started": "2024-03-27T02:46:36.936528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_config:\n",
      "  train_file: train.json\n",
      "  val_file: dev.json\n",
      "  test_file: dev.json\n",
      "  num_proc: 24\n",
      "max_input_length: 128\n",
      "max_output_length: 256\n",
      "training_args:\n",
      "  # see `transformers.Seq2SeqTrainingArguments`\n",
      "  output_dir: ./output\n",
      "  max_steps: 50000\n",
      "  # settings for data loading\n",
      "  per_device_train_batch_size: 4\n",
      "  dataloader_num_workers: 24\n",
      "  remove_unused_columns: false\n",
      "  # settings for saving checkpoints\n",
      "  save_strategy: steps\n",
      "  save_steps: 5000\n",
      "  # settings for logging\n",
      "  log_level: info\n",
      "  logging_strategy: steps\n",
      "  logging_steps: 1000\n",
      "  # settings for evaluation\n",
      "  per_device_eval_batch_size: 16\n",
      "  evaluation_strategy: steps\n",
      "  eval_steps: 2500\n",
      "  # settings for optimizer\n",
      "  # adam_epsilon: 1e-6\n",
      "  # uncomment the following line to detect nan or inf values\n",
      "  # debug: underflow_overflow\n",
      "  predict_with_generate: true\n",
      "  # see `transformers.GenerationConfig`\n",
      "  generation_config:\n",
      "    max_new_tokens: 256\n",
      "  # set your absolute deepspeed path here\n",
      "  #deepspeed: ds_zero_2.json\n",
      "  # set to true if train with cpu.\n",
      "  use_cpu: false\n",
      "peft_config:\n",
      "  peft_type: LORA\n",
      "  task_type: CAUSAL_LM\n",
      "  r: 8\n",
      "  lora_alpha: 32\n",
      "  lora_dropout: 0.1\n"
     ]
    }
   ],
   "source": [
    "!cat configs/lora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c87410a24d844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T06:44:56.043246Z",
     "start_time": "2024-01-18T05:05:28.425374Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-26T15:17:44.808873Z",
     "iopub.status.busy": "2024-03-26T15:17:44.808386Z",
     "iopub.status.idle": "2024-03-27T02:40:20.396968Z",
     "shell.execute_reply": "2024-03-27T02:40:20.395479Z",
     "shell.execute_reply.started": "2024-03-26T15:17:44.808829Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:02<00:00,  3.29it/s]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "{'loss': 3.5612, 'learning_rate': 4.9e-05, 'epoch': 0.03}                       \n",
      "{'loss': 3.3943, 'learning_rate': 4.8e-05, 'epoch': 0.07}                       \n",
      "  5%|█▊                                 | 2500/50000 [32:58<10:13:03,  1.29it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:10<00:10,  5.45s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:21<00:07,  7.64s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:26<00:00,  6.65s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.715 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 30.548849999999998, 'eval_rouge-2': 6.584894, 'eval_rouge-l': 22.783039999999996, 'eval_bleu-4': 0.032992676109196206, 'eval_runtime': 39.0564, 'eval_samples_per_second': 1.28, 'eval_steps_per_second': 0.102, 'epoch': 0.09}\n",
      "  5%|█▊                                 | 2500/50000 [33:37<10:13:03,  1.29it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:27<00:00,  6.65s/it]\u001b[A\n",
      "{'loss': 3.3684, 'learning_rate': 4.7e-05, 'epoch': 0.1}                        \u001b[A\n",
      "{'loss': 3.3415, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.14}         \n",
      "{'loss': 3.3207, 'learning_rate': 4.5e-05, 'epoch': 0.17}                       \n",
      " 10%|███▎                             | 5000/50000 [1:06:47<10:12:11,  1.23it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.44s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:15<00:05,  5.87s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.143224000000004, 'eval_rouge-2': 6.787229999999999, 'eval_rouge-l': 25.205273999999996, 'eval_bleu-4': 0.03624484309506492, 'eval_runtime': 30.8576, 'eval_samples_per_second': 1.62, 'eval_steps_per_second': 0.13, 'epoch': 0.17}\n",
      " 10%|███▎                             | 5000/50000 [1:07:18<10:12:11,  1.23it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:26<00:00,  7.63s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-5000\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 3.2894, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.21}        \n",
      "{'loss': 3.2697, 'learning_rate': 4.3e-05, 'epoch': 0.24}                       \n",
      " 15%|█████                             | 7500/50000 [1:40:34<9:14:51,  1.28it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:11<00:11,  5.53s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:04,  4.74s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.527950000000004, 'eval_rouge-2': 8.251152000000001, 'eval_rouge-l': 25.31751, 'eval_bleu-4': 0.03736489947627322, 'eval_runtime': 36.8985, 'eval_samples_per_second': 1.355, 'eval_steps_per_second': 0.108, 'epoch': 0.26}\n",
      " 15%|█████                             | 7500/50000 [1:41:11<9:14:51,  1.28it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:25<00:00,  6.86s/it]\u001b[A\n",
      "{'loss': 3.2778, 'learning_rate': 4.2e-05, 'epoch': 0.28}                       \u001b[A\n",
      "{'loss': 3.2417, 'learning_rate': 4.1e-05, 'epoch': 0.31}                       \n",
      "{'loss': 3.2111, 'learning_rate': 4e-05, 'epoch': 0.35}                         \n",
      " 20%|██████▌                          | 10000/50000 [2:14:23<8:29:16,  1.31it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:11<00:11,  5.54s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:15<00:04,  4.95s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.803302, 'eval_rouge-2': 8.15414, 'eval_rouge-l': 25.187772000000006, 'eval_bleu-4': 0.04014528139273972, 'eval_runtime': 29.6236, 'eval_samples_per_second': 1.688, 'eval_steps_per_second': 0.135, 'epoch': 0.35}\n",
      " 20%|██████▌                          | 10000/50000 [2:14:52<8:29:16,  1.31it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:18<00:00,  4.09s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-10000\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-10000/special_tokens_map.json\n",
      "{'loss': 3.2233, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.38}        \n",
      "{'loss': 3.2042, 'learning_rate': 3.8e-05, 'epoch': 0.42}                       \n",
      " 25%|████████▎                        | 12500/50000 [2:48:14<7:34:15,  1.38it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:13<00:13,  6.84s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:27<00:09,  9.62s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.615762, 'eval_rouge-2': 6.849636, 'eval_rouge-l': 23.142676, 'eval_bleu-4': 0.03104110196305175, 'eval_runtime': 52.0649, 'eval_samples_per_second': 0.96, 'eval_steps_per_second': 0.077, 'epoch': 0.44}\n",
      " 25%|████████▎                        | 12500/50000 [2:49:07<7:34:15,  1.38it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:37<00:00,  9.91s/it]\u001b[A\n",
      "{'loss': 3.2097, 'learning_rate': 3.7e-05, 'epoch': 0.45}                       \u001b[A\n",
      "{'loss': 3.1954, 'learning_rate': 3.6e-05, 'epoch': 0.49}                       \n",
      "{'loss': 3.1862, 'learning_rate': 3.5e-05, 'epoch': 0.52}                       \n",
      " 30%|█████████▉                       | 15000/50000 [3:22:25<7:33:59,  1.28it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.07s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.56s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.47230799999999, 'eval_rouge-2': 7.559811999999999, 'eval_rouge-l': 25.662242000000003, 'eval_bleu-4': 0.038366175394431344, 'eval_runtime': 22.2869, 'eval_samples_per_second': 2.243, 'eval_steps_per_second': 0.179, 'epoch': 0.52}\n",
      " 30%|█████████▉                       | 15000/50000 [3:22:47<7:33:59,  1.28it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.85s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-15000\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-15000/special_tokens_map.json\n",
      "{'loss': 3.1773, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.56}        \n",
      "{'loss': 3.1732, 'learning_rate': 3.3e-05, 'epoch': 0.59}                       \n",
      " 35%|███████████▌                     | 17500/50000 [3:56:06<7:18:59,  1.23it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:11<00:11,  5.53s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:04,  4.75s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.129799999999996, 'eval_rouge-2': 7.918668, 'eval_rouge-l': 25.296826000000006, 'eval_bleu-4': 0.03870830927062055, 'eval_runtime': 21.5951, 'eval_samples_per_second': 2.315, 'eval_steps_per_second': 0.185, 'epoch': 0.61}\n",
      " 35%|███████████▌                     | 17500/50000 [3:56:27<7:18:59,  1.23it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:17<00:00,  3.93s/it]\u001b[A\n",
      "{'loss': 3.1656, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.63}        \u001b[A\n",
      "{'loss': 3.1767, 'learning_rate': 3.1e-05, 'epoch': 0.66}                       \n",
      "{'loss': 3.1656, 'learning_rate': 3e-05, 'epoch': 0.7}                          \n",
      " 40%|█████████████▏                   | 20000/50000 [4:29:51<7:02:32,  1.18it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:11<00:11,  5.54s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:15<00:04,  4.98s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.19639599999999, 'eval_rouge-2': 8.370326, 'eval_rouge-l': 25.60831600000001, 'eval_bleu-4': 0.03922092616081649, 'eval_runtime': 29.5145, 'eval_samples_per_second': 1.694, 'eval_steps_per_second': 0.136, 'epoch': 0.7}\n",
      " 40%|█████████████▏                   | 20000/50000 [4:30:21<7:02:32,  1.18it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:17<00:00,  4.03s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-20000\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-20000/special_tokens_map.json\n",
      "{'loss': 3.1676, 'learning_rate': 2.9e-05, 'epoch': 0.73}                       \n",
      "{'loss': 3.1575, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.77}        \n",
      " 45%|██████████████▊                  | 22500/50000 [5:03:50<5:45:53,  1.33it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.26s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:02,  2.93s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.253755999999996, 'eval_rouge-2': 7.98341, 'eval_rouge-l': 25.806214, 'eval_bleu-4': 0.03706896571438134, 'eval_runtime': 22.9169, 'eval_samples_per_second': 2.182, 'eval_steps_per_second': 0.175, 'epoch': 0.79}\n",
      " 45%|██████████████▊                  | 22500/50000 [5:04:13<5:45:53,  1.33it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:11<00:00,  2.87s/it]\u001b[A\n",
      "{'loss': 3.1669, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.8}         \u001b[A\n",
      "{'loss': 3.1388, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.84}        \n",
      "{'loss': 3.1386, 'learning_rate': 2.5e-05, 'epoch': 0.87}                       \n",
      " 50%|████████████████▌                | 25000/50000 [5:37:33<5:16:45,  1.32it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.30s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:02,  2.82s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.607604, 'eval_rouge-2': 8.133978, 'eval_rouge-l': 25.529892, 'eval_bleu-4': 0.040477483263578945, 'eval_runtime': 22.7668, 'eval_samples_per_second': 2.196, 'eval_steps_per_second': 0.176, 'epoch': 0.87}\n",
      " 50%|████████████████▌                | 25000/50000 [5:37:56<5:16:45,  1.32it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:11<00:00,  2.84s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-25000\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-25000/special_tokens_map.json\n",
      "{'loss': 3.1351, 'learning_rate': 2.4e-05, 'epoch': 0.91}                       \n",
      "{'loss': 3.1296, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.94}        \n",
      " 55%|██████████████████▏              | 27500/50000 [6:11:21<5:03:25,  1.24it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:11<00:11,  5.51s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:15<00:04,  4.95s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.207152, 'eval_rouge-2': 7.835595999999999, 'eval_rouge-l': 26.349768000000005, 'eval_bleu-4': 0.038002815587862344, 'eval_runtime': 23.3727, 'eval_samples_per_second': 2.139, 'eval_steps_per_second': 0.171, 'epoch': 0.96}\n",
      " 55%|██████████████████▏              | 27500/50000 [6:11:44<5:03:25,  1.24it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:19<00:00,  4.58s/it]\u001b[A\n",
      "{'loss': 3.1336, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.98}        \u001b[A\n",
      "{'loss': 3.1211, 'learning_rate': 2.1e-05, 'epoch': 1.01}                       \n",
      "{'loss': 3.1168, 'learning_rate': 2e-05, 'epoch': 1.05}                         \n",
      " 60%|███████████████████▊             | 30000/50000 [6:45:06<4:19:20,  1.29it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.37s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:02,  2.83s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.8215, 'eval_rouge-2': 7.949440000000001, 'eval_rouge-l': 27.194873999999995, 'eval_bleu-4': 0.03932845173900562, 'eval_runtime': 22.4262, 'eval_samples_per_second': 2.23, 'eval_steps_per_second': 0.178, 'epoch': 1.05}\n",
      " 60%|███████████████████▊             | 30000/50000 [6:45:28<4:19:20,  1.29it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.70s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-30000\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-30000/special_tokens_map.json\n",
      "{'loss': 3.0911, 'learning_rate': 1.9e-05, 'epoch': 1.08}                       \n",
      "{'loss': 3.1071, 'learning_rate': 1.8e-05, 'epoch': 1.12}                       \n",
      " 65%|█████████████████████▍           | 32500/50000 [7:19:02<3:36:28,  1.35it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.41s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:05,  5.57s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.356156, 'eval_rouge-2': 7.808882, 'eval_rouge-l': 25.303856, 'eval_bleu-4': 0.038480883858219274, 'eval_runtime': 29.1707, 'eval_samples_per_second': 1.714, 'eval_steps_per_second': 0.137, 'epoch': 1.13}\n",
      " 65%|█████████████████████▍           | 32500/50000 [7:19:31<3:36:28,  1.35it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:17<00:00,  4.46s/it]\u001b[A\n",
      "{'loss': 3.1103, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.15}        \u001b[A\n",
      "{'loss': 3.0942, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.19}        \n",
      "{'loss': 3.0996, 'learning_rate': 1.5e-05, 'epoch': 1.22}                       \n",
      " 70%|███████████████████████          | 35000/50000 [7:53:00<3:27:49,  1.20it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.22s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:15<00:05,  5.81s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.604214, 'eval_rouge-2': 8.090531999999998, 'eval_rouge-l': 26.033706000000002, 'eval_bleu-4': 0.03916715902485951, 'eval_runtime': 29.4876, 'eval_samples_per_second': 1.696, 'eval_steps_per_second': 0.136, 'epoch': 1.22}\n",
      " 70%|███████████████████████          | 35000/50000 [7:53:29<3:27:49,  1.20it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:17<00:00,  4.56s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-35000\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-35000/special_tokens_map.json\n",
      "{'loss': 3.0859, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.26}        \n",
      "{'loss': 3.102, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.29}         \n",
      " 75%|████████████████████████▊        | 37500/50000 [8:26:50<2:46:53,  1.25it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.39s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:02,  2.87s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.028043999999994, 'eval_rouge-2': 7.65972, 'eval_rouge-l': 26.199838, 'eval_bleu-4': 0.03959396068757788, 'eval_runtime': 14.8396, 'eval_samples_per_second': 3.369, 'eval_steps_per_second': 0.27, 'epoch': 1.31}\n",
      " 75%|████████████████████████▊        | 37500/50000 [8:27:04<2:46:53,  1.25it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.58s/it]\u001b[A\n",
      "{'loss': 3.1076, 'learning_rate': 1.2e-05, 'epoch': 1.33}                       \u001b[A\n",
      "{'loss': 3.1119, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.36}        \n",
      "{'loss': 3.0846, 'learning_rate': 1e-05, 'epoch': 1.4}                          \n",
      " 80%|██████████████████████████▍      | 40000/50000 [9:00:35<2:04:18,  1.34it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.18s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:15<00:05,  5.72s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.570636, 'eval_rouge-2': 7.856097999999999, 'eval_rouge-l': 25.979128, 'eval_bleu-4': 0.039571862573273366, 'eval_runtime': 21.7806, 'eval_samples_per_second': 2.296, 'eval_steps_per_second': 0.184, 'epoch': 1.4}\n",
      " 80%|██████████████████████████▍      | 40000/50000 [9:00:57<2:04:18,  1.34it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:17<00:00,  4.53s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-40000\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-40000/special_tokens_map.json\n",
      "{'loss': 3.0983, 'learning_rate': 9e-06, 'epoch': 1.43}                         \n",
      "{'loss': 3.0906, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.47}         \n",
      " 85%|████████████████████████████     | 42500/50000 [9:34:23<1:34:13,  1.33it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.34s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:09<00:03,  3.38s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.450656, 'eval_rouge-2': 7.810104, 'eval_rouge-l': 25.800806, 'eval_bleu-4': 0.038470006912349045, 'eval_runtime': 16.741, 'eval_samples_per_second': 2.987, 'eval_steps_per_second': 0.239, 'epoch': 1.48}\n",
      " 85%|████████████████████████████     | 42500/50000 [9:34:40<1:34:13,  1.33it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:12<00:00,  3.08s/it]\u001b[A\n",
      "{'loss': 3.1056, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.5}          \u001b[A\n",
      "{'loss': 3.0919, 'learning_rate': 6e-06, 'epoch': 1.54}                         \n",
      "{'loss': 3.0763, 'learning_rate': 5e-06, 'epoch': 1.57}                         \n",
      " 90%|██████████████████████████████▌   | 45000/50000 [10:08:13<57:45,  1.44it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.17s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.58s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.871484, 'eval_rouge-2': 7.956433999999999, 'eval_rouge-l': 26.209339999999994, 'eval_bleu-4': 0.03773074216151202, 'eval_runtime': 15.0432, 'eval_samples_per_second': 3.324, 'eval_steps_per_second': 0.266, 'epoch': 1.57}\n",
      " 90%|██████████████████████████████▌   | 45000/50000 [10:08:28<57:45,  1.44it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.83s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-45000\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-45000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-45000/special_tokens_map.json\n",
      "{'loss': 3.0731, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.61}         \n",
      "{'loss': 3.0803, 'learning_rate': 3e-06, 'epoch': 1.64}                         \n",
      " 95%|████████████████████████████████▎ | 47500/50000 [10:41:56<32:01,  1.30it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.96s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:10<00:03,  3.59s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.187002, 'eval_rouge-2': 7.945428, 'eval_rouge-l': 26.203610000000005, 'eval_bleu-4': 0.03851936763939023, 'eval_runtime': 26.9661, 'eval_samples_per_second': 1.854, 'eval_steps_per_second': 0.148, 'epoch': 1.66}\n",
      " 95%|████████████████████████████████▎ | 47500/50000 [10:42:23<32:01,  1.30it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:12<00:00,  3.05s/it]\u001b[A\n",
      "{'loss': 3.0818, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.68}        \u001b[A\n",
      "{'loss': 3.0688, 'learning_rate': 1.0000000000000002e-06, 'epoch': 1.71}        \n",
      "{'loss': 3.0782, 'learning_rate': 0.0, 'epoch': 1.75}                           \n",
      "100%|██████████████████████████████████| 50000/50000 [11:16:00<00:00,  1.27it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:06<00:06,  3.01s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:09<00:03,  3.09s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.883421999999996, 'eval_rouge-2': 8.248144, 'eval_rouge-l': 26.764362000000006, 'eval_bleu-4': 0.04060319521153886, 'eval_runtime': 16.1733, 'eval_samples_per_second': 3.092, 'eval_steps_per_second': 0.247, 'epoch': 1.75}\n",
      "100%|██████████████████████████████████| 50000/50000 [11:16:16<00:00,  1.27it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:11<00:00,  2.90s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-50000\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/tmp-checkpoint-50000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 40576.5078, 'train_samples_per_second': 4.929, 'train_steps_per_second': 1.232, 'train_loss': 3.1679529296875, 'epoch': 1.75}\n",
      "100%|██████████████████████████████████| 50000/50000 [11:16:16<00:00,  1.23it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 16\n",
      "100%|███████████████████████████████████████████| 67/67 [05:59<00:00,  5.36s/it]\n"
     ]
    }
   ],
   "source": [
    "!export python_dir='/home/yuxiang/miniconda3/envs/py311_llm_dev/bin' \\\n",
    "&& export base_model_dir='/home/yuxiang/local/ai_workspace/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/f30825950ce00cb0577bf6a15e0d95de58e328dc' \\\n",
    "&& export CUDA_VISIBLE_DEVICES=0 \\\n",
    "&& ${python_dir}/python3 finetune_hf.py  data/AdvertiseGen_fix  ${base_model_dir}  configs/lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9418f6c5c264601",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f22b735175e1c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:03:19.390123Z",
     "start_time": "2024-01-18T07:03:19.246666Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:41:51.711357Z",
     "iopub.status.busy": "2024-03-27T02:41:51.710812Z",
     "iopub.status.idle": "2024-03-27T02:41:51.840344Z",
     "shell.execute_reply": "2024-03-27T02:41:51.838839Z",
     "shell.execute_reply.started": "2024-03-27T02:41:51.711296Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-10000  checkpoint-25000  checkpoint-40000  checkpoint-50000\n",
      "checkpoint-15000  checkpoint-30000  checkpoint-45000\n",
      "checkpoint-20000  checkpoint-35000  checkpoint-5000\n"
     ]
    }
   ],
   "source": [
    "!ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5060015c24e97ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:08:13.616364Z",
     "start_time": "2024-01-18T07:07:07.346906Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-27T02:43:20.026294Z",
     "iopub.status.busy": "2024-03-27T02:43:20.025778Z",
     "iopub.status.idle": "2024-03-27T02:43:35.218606Z",
     "shell.execute_reply": "2024-03-27T02:43:35.217289Z",
     "shell.execute_reply.started": "2024-03-27T02:43:20.026238Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:03<00:00,  2.31it/s]\n",
      "这款连衣裙采用网纱和百褶的拼接设计，带出女性柔美的气质。套头的设计，方便穿脱。拼接的木耳边，显得性感十足。前短后长的裙摆，修饰身材，拉长身高。前襟采用拉链设计，方便穿脱。后背压褶设计，立体有型，不规则的裙摆，彰显出个性。\n"
     ]
    }
   ],
   "source": [
    "!export python_dir='/home/yuxiang/miniconda3/envs/py311_llm_dev/bin' \\\n",
    "&& export CUDA_VISIBLE_DEVICES=0 \\\n",
    "&& ${python_dir}/python3  inference_hf.py output/checkpoint-50000/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
